import json
import os
import re
import numpy as np
from pathlib import Path
from collections import defaultdict
from gensim.models import KeyedVectors
from sklearn.cluster import DBSCAN
from tqdm import tqdm
import mmap

class IndustryAggregator:
    def __init__(self, w2v_path, mapping_file=None):
        # åŠ è½½è¯å‘é‡æ¨¡å‹
        self.w2v = KeyedVectors.load(w2v_path)
        
        # è¡Œä¸šæ˜ å°„é…ç½®
        self.mapping_file = mapping_file or "industry_mapping.json"
        self.industry_mapping = {}
        self._load_mapping()
        
        # é¢„å®šä¹‰è¡Œä¸šåç¼€
        self.suffix_pattern = re.compile(r'(è¡Œä¸š|ä¸š|äº§ä¸š)$')
        
        # èšç±»å‚æ•°
        self.cluster_params = {
            'eps': 0.3,  # èšç±»åŠå¾„
            'min_samples': 2,  # æœ€å°æ ·æœ¬æ•°
            'metric': 'cosine'  # ä½™å¼¦ç›¸ä¼¼åº¦
        }

    def _load_mapping(self):
        """åŠ è½½å·²æœ‰çš„è¡Œä¸šæ˜ å°„"""
        if os.path.exists(self.mapping_file):
            with open(self.mapping_file, 'r', encoding='utf-8') as f:
                self.industry_mapping = json.load(f)

    def _save_mapping(self):
        """ä¿å­˜è¡Œä¸šæ˜ å°„"""
        with open(self.mapping_file, 'w', encoding='utf-8') as f:
            json.dump(self.industry_mapping, f, ensure_ascii=False, indent=4)

    def is_contained_in(self, sub_name, full_name):
        """åˆ¤æ–­sub_nameæ˜¯å¦æ˜¯full_nameçš„å‰ç¼€æˆ–åç¼€"""
        # å»é™¤åç¼€ï¼ˆä¾‹å¦‚â€œè¡Œä¸šâ€ã€â€œä¸šâ€ï¼‰
        sub_name = self.suffix_pattern.sub('', sub_name)
        full_name = self.suffix_pattern.sub('', full_name)
        
        # åˆ¤æ–­sub_nameæ˜¯å¦ä¸ºfull_nameçš„å‰ç¼€æˆ–åç¼€
        return sub_name in full_name
    
    def optimize_standard_names(self):
        """ç¡®ä¿æ¯ä¸ªæ ‡å‡†åç§°æ˜¯è¯¥ç»„ä¸­æœ€çŸ­çš„åç§°"""
        # æ„å»ºæ ‡å‡†åç§°åˆ°åŸå§‹åç§°åˆ—è¡¨çš„æ˜ å°„
        std_to_raws = defaultdict(list)
        for raw_name, std_name in self.industry_mapping.items():
            std_to_raws[std_name].append(raw_name)
        
        # åˆ›å»ºæ–°æ˜ å°„å­—å…¸
        new_mapping = {}
        updated = False
        
        for std_name, raw_names in std_to_raws.items():
            # æ‰¾åˆ°ç»„å†…æœ€çŸ­çš„åç§°ï¼ˆåŒ…æ‹¬å½“å‰æ ‡å‡†åç§°ï¼‰
            all_names_in_group = raw_names + [std_name]
            # å»é™¤â€œè¡Œä¸šâ€å’Œâ€œä¸šâ€åå†æ¯”è¾ƒé•¿åº¦
            stripped_names = [self.suffix_pattern.sub('', name) for name in all_names_in_group]
            shortest_name = min(stripped_names, key=lambda x: len(x.strip()))
            
            # å¦‚æœå½“å‰æ ‡å‡†åç§°ä¸æ˜¯æœ€çŸ­çš„ï¼Œåˆ™ä½¿ç”¨æœ€çŸ­çš„åç§°ä½œä¸ºæ–°çš„æ ‡å‡†åç§°
            new_std_name = shortest_name if len(shortest_name.strip()) < len(std_name.strip()) else std_name
            
            # å¦‚æœæ ‡å‡†åç§°æœ‰å˜åŒ–ï¼Œæ ‡è®°éœ€è¦æ›´æ–°
            if new_std_name != std_name:
                updated = True
            
            # æ›´æ–°æ˜ å°„å…³ç³»
            for raw_name in raw_names:
                new_mapping[raw_name] = new_std_name
        
        if updated:
            self.industry_mapping = new_mapping
        return updated
    
    def _get_word_vector(self, word):
        """å®‰å…¨è·å–è¯å‘é‡ï¼Œå¤„ç†æœªç™»å½•è¯"""
        # å…ˆå°è¯•ç›´æ¥è·å–
        if word in self.w2v:
            return self.w2v[word]
            
        # å°è¯•å»é™¤åç¼€åå†æŸ¥è¯¢
        base_word = self.suffix_pattern.sub('', word)
        if base_word in self.w2v:
            return self.w2v[base_word]
            
        # å­—ç¬¦çº§å›é€€æ–¹æ¡ˆ
        chars = [c for c in base_word if c in self.w2v]
        if chars:
            return np.mean([self.w2v[c] for c in chars], axis=0)
            
        # æœ€ç»ˆå›é€€åˆ°é›¶å‘é‡
        return np.zeros(self.w2v.vector_size)

    def _cluster_industries(self, industry_names):
        """ä½¿ç”¨è¯å‘é‡èšç±»è¡Œä¸šåç§°"""
        # è·å–æ‰€æœ‰è¯å‘é‡
        vectors = [self._get_word_vector(name) for name in industry_names]
        
        # DBSCANèšç±»
        clustering = DBSCAN(
            eps=self.cluster_params['eps'],
            min_samples=self.cluster_params['min_samples'],
            metric=self.cluster_params['metric']
        ).fit(vectors)
        
        # æ„å»ºèšç±»ç»“æœ
        clusters = defaultdict(list)
        for idx, label in enumerate(clustering.labels_):
            if label != -1:  # å¿½ç•¥å™ªå£°ç‚¹
                clusters[label].append(industry_names[idx])
                
        return clusters

    def aggregate(self, industry_names):
        """ä¸¤é˜¶æ®µèšåˆï¼šè§„åˆ™åŒ¹é… + è¯­ä¹‰èšç±»"""
        # ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºè§„åˆ™çš„ç²¾ç¡®åŒ¹é…
        rule_based = defaultdict(list)
        for name in industry_names:
            base_name = self.suffix_pattern.sub('', name)
            rule_based[base_name].append(name)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¯¹å‰©ä½™æœªåŒ¹é…çš„è¿›è¡Œè¯­ä¹‰èšç±»
        all_clustered = {}
        for base_name, variants in rule_based.items():
            if len(variants) == 1:  # å•ä¸ªåç§°éœ€è¦è¯­ä¹‰åŒ¹é…
                clusters = self._cluster_industries([base_name] + list(self.industry_mapping.keys()))
                if clusters:
                    # åˆå¹¶åˆ°æœ€æ¥è¿‘çš„å·²æœ‰ç±»åˆ«
                    closest = next(iter(clusters.values()))[0]
                    all_clustered[base_name] = closest
                else:
                    # ä½œä¸ºæ–°ç±»åˆ«
                    all_clustered[base_name] = base_name
            else:
                # å¤šä¸ªå˜ä½“ä½¿ç”¨åŸºç¡€åç§°ä½œä¸ºæ ‡å‡†
                all_clustered[base_name] = base_name
                
            # æ›´æ–°æ˜ å°„å…³ç³»
            for variant in variants:
                # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦å­˜åœ¨åŒ…å«å…³ç³»
                for existing_name in list(self.industry_mapping.keys()):
                    if self.is_contained_in(variant, existing_name):
                        self.industry_mapping[variant] = self.industry_mapping[existing_name]
                        break
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ…å«å…³ç³»ï¼Œå°±ç»´æŒåŸæ ·
                    self.industry_mapping[variant] = base_name
                all_clustered[base_name] = self.industry_mapping[variant]
        
        return all_clustered



def extract_json_objects(file_path):
    """ä½¿ç”¨å†…å­˜æ˜ å°„é«˜æ•ˆæå– JSON å¯¹è±¡ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""
    with open(file_path, 'r+b') as f:
        file_size = f.seek(0, 2)
        f.seek(0)
        
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            # éè´ªå©ªåŒ¹é…å¤§æ‹¬å·åŒ…è£¹çš„ JSON å—ï¼ˆå°½å¯èƒ½å°‘ï¼‰
            pattern = re.compile(rb'\{.*?\}(?=\s*\{|[\s\S]*$)', re.DOTALL)
            
            with tqdm(total=file_size, desc="ğŸ” æ­£åœ¨è§£æ JSON", unit="B", unit_scale=True) as pbar:
                last_pos = 0
                for match in pattern.finditer(mm):
                    start, end = match.span()
                    chunk = match.group()
                    try:
                        obj = json.loads(chunk.decode('utf-8'))
                        yield obj
                    except (json.JSONDecodeError, UnicodeDecodeError):
                        pass  # å¿½ç•¥è§£æå¤±è´¥çš„å—
                    finally:
                        pbar.update(end - last_pos)
                        last_pos = end
                        
                # è¡¥å…¨å‰©ä½™è¿›åº¦
                pbar.update(file_size - last_pos)


def process_industry_data(input_file, output_dir, aggregator):
    """å¤„ç†è¡Œä¸šæ•°æ®"""
    
    # æå–è¡Œä¸šåç§°å’Œå› æœå…³ç³»
    industry_data = defaultdict(list)
    industry_names = set()
    for data in extract_json_objects(input_file):
        try:
            industry = data['industry']
            industry_names.add(industry)
            industry_data[industry].append(data['cause and effect'])
        except KeyError:
            continue
    
    # æ‰§è¡Œèšåˆ
    aggregator.aggregate(industry_names)
    
    # ä¼˜åŒ–æ ‡å‡†åç§°
    aggregator.optimize_standard_names()
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    Path(output_dir).mkdir(exist_ok=True)
    output_files = {}
    all_filenames = set()  # æ–°å¢ï¼šä¿å­˜æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶å
    
    for raw_name, items in industry_data.items():
        std_name = aggregator.industry_mapping.get(raw_name, raw_name)
        safe_name = re.sub(r'[\\/*?:"<>|]', '_', std_name)
        safe_name = safe_name + "ä¸š"  # æ·»åŠ "ä¸š"åç¼€
        out_path = Path(output_dir) / f"{safe_name}.txt"
        
        with open(out_path, 'a', encoding='utf-8') as f:
            f.write("\n".join(items) + "\n")
        
        output_files[std_name] = out_path
        all_filenames.add(safe_name)  # è®°å½•æ–‡ä»¶åï¼ˆä¸å«åç¼€ï¼‰

    
    # ä¿å­˜æ‰€æœ‰æ–‡ä»¶ååˆ°txtæ–‡ä»¶ï¼ˆæ–°å¢éƒ¨åˆ†ï¼‰
    filename_list_path = Path(output_dir) / "generated_files_list.txt"
    with open(filename_list_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(all_filenames))
    
    # ä¿å­˜æ›´æ–°åçš„æ˜ å°„
    aggregator._save_mapping()
    return output_files

# ä¿®æ”¹ä¸»ç¨‹åºéƒ¨åˆ†
if __name__ == "__main__":
    BASE_DIR = Path(__file__).resolve().parent.parent
    CURRENT_DIR = Path(__file__).resolve().parent
    # åˆå§‹åŒ–èšåˆå™¨
    aggregator = IndustryAggregator(
        w2v_path=os.path.join(CURRENT_DIR, 'ChineseEmbedding.bin'),
        mapping_file=os.path.join(CURRENT_DIR, 'industry_mapping.json')
    )
    
    # å¤„ç†æ•°æ®
    input_file = os.path.join(BASE_DIR, "data", "cause_and_effect.txt")
    output_dir = os.path.join(BASE_DIR, "data", "å› æœå…³ç³»")
    
    result = process_industry_data(input_file, output_dir, aggregator)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    std_name_counts = defaultdict(list)
    for raw_name, std_name in aggregator.industry_mapping.items():
        std_name_counts[std_name].append(raw_name)
    
    print(f"ç”Ÿæˆ {len(result)} ä¸ªè¡Œä¸šåˆ†ç±»æ–‡ä»¶")
    print("æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶åå·²ä¿å­˜åˆ°: generated_files_list.txt")
    
    # æ‰“å°å¤šå¯¹ä¸€æ˜ å°„å…³ç³»
    multi_mapping_count = 0
    print("\nå¤šå¯¹ä¸€æ˜ å°„å…³ç³»:")
    for std_name, raw_names in sorted(std_name_counts.items(), key=lambda x: len(x[1]), reverse=True):
        if len(raw_names) > 1:
            multi_mapping_count += 1
            print(f"\næ ‡å‡†åç§°: {std_name} (åŒ…å« {len(raw_names)} ä¸ªå˜ä½“)")
            for raw_name in raw_names:
                print(f"  - {raw_name}")
    
    print(f"\næ€»è®¡å‘ç° {multi_mapping_count} ä¸ªå¤šå¯¹ä¸€æ˜ å°„ç»„")